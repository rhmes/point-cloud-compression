{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhmes/point-cloud-compression/blob/main/point_cloud_compression_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4161fdd0",
      "metadata": {
        "id": "4161fdd0"
      },
      "source": [
        "# Point Cloud Compression Demo\n",
        "\n",
        "This notebook demonstrates installation, core functionality, and testing for the point cloud compression project."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a81a4a6b",
      "metadata": {
        "id": "a81a4a6b"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "This section prepares the environment for the point cloud compression project:\n",
        "\n",
        "1.  **Mount Google Drive**: Connects Colab to Google Drive for persistent storage of the repository and virtual environment.\n",
        "2.  **Clone/Update Repository**: Clones the project repository from GitHub to Google Drive or updates it if it already exists.\n",
        "3.  **Virtual Environment Setup**: Creates and populates a virtual environment (`venv_gpu` or `venv_cpu`) in Google Drive using a setup script, only if it doesn't exist.\n",
        "4.  **Prepare Environment**: Modifies Python's `sys.path` to include the virtual environment's packages, allowing the notebook to use installed dependencies across sessions.\n",
        "5.  **Verify Dependencies**: Checks if key libraries from the virtual environment can be imported successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "daa1006f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "daa1006f",
        "outputId": "7cdfc45f-a3f7-4def-98c7-44fa98e1d407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted.\n"
          ]
        }
      ],
      "source": [
        "# Connect to Google Drive (for Colab users)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print('Google Drive mounted.')\n",
        "except ImportError:\n",
        "    print('Not running in Colab, skipping Google Drive mount.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a48bcdf3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a48bcdf3",
        "outputId": "63e60ab4-6a14-4d7e-a269-9b0977c5f964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating point-cloud-compression...\n",
            "Current working directory: /content/drive/MyDrive/projects/point-cloud-compression\n"
          ]
        }
      ],
      "source": [
        "# Clone repo only if not already cloned\n",
        "import os\n",
        "repo_url = 'https://github.com/rhmes/point-cloud-compression.git'\n",
        "parent_dir = '/content/drive/MyDrive/projects'\n",
        "repo_dir = os.path.join(parent_dir, 'point-cloud-compression')\n",
        "os.makedirs(parent_dir, exist_ok=True)\n",
        "os.chdir(parent_dir)\n",
        "if not os.path.exists(repo_dir):\n",
        "  print('Cloning point-cloud-compression...')\n",
        "  !git clone {repo_url}\n",
        "else:\n",
        "  print('Updating point-cloud-compression...')\n",
        "  !cd {repo_dir} && git pull\n",
        "os.chdir(repo_dir)\n",
        "print(f'Current working directory: {os.getcwd()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1a3fd280",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1a3fd280",
        "outputId": "073b927d-a526-4e32-d37d-d654f96a7de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up GPU virtual environment...\n",
            "No virtual environment found at venv_gpu. Running setup script...\n",
            "Requirement already satisfied: virtualenv in /usr/local/lib/python3.12/dist-packages (20.34.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.12/dist-packages (from virtualenv) (0.4.0)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.12/dist-packages (from virtualenv) (3.19.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from virtualenv) (4.3.8)\n",
            "created virtual environment CPython3.10.12.final.0-64 in 12277ms\n",
            "  creator CPython3Posix(dest=/content/drive/MyDrive/projects/point-cloud-compression/venv_gpu, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: MarkupSafe==3.0.2, filelock==3.18.0, fsspec==2025.7.0, jinja2==3.1.6, mpmath==1.3.0, networkx==3.4.2, numpy==2.2.6, nvidia_cublas_cu12==12.1.3.1, nvidia_cuda_cupti_cu12==12.1.105, nvidia_cuda_nvrtc_cu12==12.1.105, nvidia_cuda_runtime_cu12==12.1.105, nvidia_cudnn_cu12==8.9.2.26, nvidia_cufft_cu12==11.0.2.54, nvidia_curand_cu12==10.3.2.106, nvidia_cusolver_cu12==11.4.5.107, nvidia_cusparse_cu12==12.1.0.106, nvidia_nccl_cu12==2.20.5, nvidia_nvjitlink_cu12==12.9.86, nvidia_nvtx_cu12==12.1.105, pandas==2.3.2, pillow==11.3.0, pip==25.2, plyfile==1.1.2, pyntcloud==0.3.1, python_dateutil==2.9.0.post0, pytz==2025.2, scipy==1.15.3, setuptools==80.9.0, six==1.17.0, sympy==1.14.0, torch==2.3.0+cu121, torchac==0.9.3, torchvision==0.18.0+cu121, tqdm==4.67.1, triton==2.3.0, typing_extensions==4.14.1, tzdata==2025.2\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n",
            "Activating virtual environment...\n",
            "Installing dependencies from requirements_gpu.txt...\n",
            "venv_python=/content/drive/MyDrive/projects/point-cloud-compression/venv_gpu/bin/python\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu121\n",
            "Looking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt210/download.html\n",
            "Requirement already satisfied: numpy in ./venv_gpu/lib/python3.10/site-packages (from -r requirements_gpu.txt (line 2)) (2.2.6)\n",
            "Requirement already satisfied: filelock==3.18.0 in ./venv_gpu/lib/python3.10/site-packages (from -r requirements_gpu.txt (line 3)) (3.18.0)\n",
            "Requirement already satisfied: fsspec==2025.7.0 in ./venv_gpu/lib/python3.10/site-packages (from -r requirements_gpu.txt (line 4)) (2025.7.0)\n",
            "Requirement already satisfied: Jinja2==3.1.6 in ./venv_gpu/lib/python3.10/site-packages (from -r requirements_gpu.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe==3.0.2 in ./venv_gpu/lib/python3.10/site-packages (from -r requirements_gpu.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: mpmath==1.3.0 in ./venv_gpu/lib/python3.10/site-packages (from -r requirements_gpu.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: networkx in ./venv_gpu/lib/python3.10/site-packages (from -r requirements_gpu.txt (line 8)) (3.4.2)\n",
            "Requirement already satisfied: setuptools==80.9.0 in ./venv_gpu/lib/python3.10/site-packages (from -r requirements_gpu.txt (line 9)) (80.9.0)\n",
            "Requirement already satisfied: sympy==1.14.0 in ./venv_gpu/lib/python3.10/site-packages (from -r requirements_gpu.txt (line 10)) (1.14.0)\n",
            "Requirement already satisfied: tqdm==4.67.1 in ./venv_gpu/lib/python3.10/site-packages (from -r requirements_gpu.txt (line 11)) (4.67.1)\n",
            "Collecting typing_extensions==4.12.2 (from -r requirements_gpu.txt (line 12))\n",
            "  Downloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pandas==1.5.3 (from -r requirements_gpu.txt (line 13))\n",
            "  Using cached pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scikit-learn==1.4.0 (from -r requirements_gpu.txt (line 14))\n",
            "  Using cached scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pyyaml (from -r requirements_gpu.txt (line 15))\n",
            "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: pyntcloud in ./venv_gpu/lib/python3.10/site-packages (from -r requirements_gpu.txt (line 18)) (0.3.1)\n",
            "Requirement already satisfied: plyfile in ./venv_gpu/lib/python3.10/site-packages (from -r requirements_gpu.txt (line 19)) (1.1.2)\n",
            "Collecting trimesh (from -r requirements_gpu.txt (line 20))\n",
            "  Using cached trimesh-4.7.4-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting open3d (from -r requirements_gpu.txt (line 21))\n",
            "  Downloading open3d-0.19.0-cp310-cp310-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting torch==2.1.0+cu121 (from -r requirements_gpu.txt (line 24))\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.1.0%2Bcu121-cp310-cp310-linux_x86_64.whl (2200.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m  \u001b[33m0:00:42\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.16.0+cu121 (from -r requirements_gpu.txt (line 25))\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.16.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.1.0+cu121 (from -r requirements_gpu.txt (line 26))\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.1.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchac==0.9.3 in ./venv_gpu/lib/python3.10/site-packages (from -r requirements_gpu.txt (line 30)) (0.9.3)\n",
            "Collecting pytorch3d==0.7.5 (from -r requirements_gpu.txt (line 33))\n",
            "  Downloading https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt210/pytorch3d-0.7.5-cp310-cp310-linux_x86_64.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in ./venv_gpu/lib/python3.10/site-packages (from pandas==1.5.3->-r requirements_gpu.txt (line 13)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./venv_gpu/lib/python3.10/site-packages (from pandas==1.5.3->-r requirements_gpu.txt (line 13)) (2025.2)\n",
            "Collecting numpy (from -r requirements_gpu.txt (line 2))\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in ./venv_gpu/lib/python3.10/site-packages (from scikit-learn==1.4.0->-r requirements_gpu.txt (line 14)) (1.15.3)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn==1.4.0->-r requirements_gpu.txt (line 14))\n",
            "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.4.0->-r requirements_gpu.txt (line 14))\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0+cu121->-r requirements_gpu.txt (line 24))\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting requests (from torchvision==0.16.0+cu121->-r requirements_gpu.txt (line 25))\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv_gpu/lib/python3.10/site-packages (from torchvision==0.16.0+cu121->-r requirements_gpu.txt (line 25)) (11.3.0)\n",
            "Collecting fvcore (from pytorch3d==0.7.5->-r requirements_gpu.txt (line 33))\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath (from pytorch3d==0.7.5->-r requirements_gpu.txt (line 33))\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dash>=2.6.0 (from open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading dash-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting werkzeug>=3.0.0 (from open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting flask>=3.0.0 (from open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting nbformat>=5.7.0 (from open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting configargparse (from open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading configargparse-1.7.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting addict (from open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting matplotlib>=3 (from open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading matplotlib-3.10.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting pyquaternion (from open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting plotly>=5.0.0 (from dash>=2.6.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading plotly-6.3.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting importlib-metadata (from dash>=2.6.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting retrying (from dash>=2.6.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting nest-asyncio (from dash>=2.6.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting blinker>=1.9.0 (from flask>=3.0.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting click>=8.1.3 (from flask>=3.0.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting itsdangerous>=2.2.0 (from flask>=3.0.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting ipython>=6.1.0 (from ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading ipython-8.37.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting traitlets>=4.3.1 (from ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting decorator (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting exceptiongroup (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting matplotlib-inline (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pexpect>4.3 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting prompt_toolkit<3.1.0,>=3.0.41 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading prompt_toolkit-3.0.51-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting pygments>=2.4.0 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting stack_data (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting wcwidth (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading parso-0.8.5-py2.py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib>=3->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib>=3->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib>=3->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading fonttools-4.59.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (108 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib>=3->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting packaging>=20.0 (from matplotlib>=3->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib>=3->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting fastjsonschema>=2.15 (from nbformat>=5.7.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading fastjsonschema-2.21.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting jsonschema>=2.6 (from nbformat>=5.7.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting jupyter-core!=5.0.*,>=4.12 (from nbformat>=5.7.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading jupyter_core-5.8.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting attrs>=22.2.0 (from jsonschema>=2.6->nbformat>=5.7.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.6->nbformat>=5.7.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=2.6->nbformat>=5.7.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=2.6->nbformat>=5.7.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading rpds_py-0.27.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting platformdirs>=2.5 (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading platformdirs-4.3.8-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting narwhals>=1.15.1 (from plotly>=5.0.0->dash>=2.6.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading narwhals-2.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: six>=1.5 in ./venv_gpu/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas==1.5.3->-r requirements_gpu.txt (line 13)) (1.17.0)\n",
            "Collecting yacs>=0.1.6 (from fvcore->pytorch3d==0.7.5->-r requirements_gpu.txt (line 33))\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Collecting termcolor>=1.1 (from fvcore->pytorch3d==0.7.5->-r requirements_gpu.txt (line 33))\n",
            "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting tabulate (from fvcore->pytorch3d==0.7.5->-r requirements_gpu.txt (line 33))\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting portalocker (from iopath->pytorch3d==0.7.5->-r requirements_gpu.txt (line 33))\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting zipp>=3.20 (from importlib-metadata->dash>=2.6.0->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->torchvision==0.16.0+cu121->-r requirements_gpu.txt (line 25))\n",
            "  Downloading charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->torchvision==0.16.0+cu121->-r requirements_gpu.txt (line 25))\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->torchvision==0.16.0+cu121->-r requirements_gpu.txt (line 25))\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->torchvision==0.16.0+cu121->-r requirements_gpu.txt (line 25))\n",
            "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting executing>=1.2.0 (from stack_data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack_data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pure-eval (from stack_data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d->-r requirements_gpu.txt (line 21))\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Downloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m135.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m141.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trimesh-4.7.4-py3-none-any.whl (709 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.8/709.8 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading open3d-0.19.0-cp310-cp310-manylinux_2_31_x86_64.whl (447.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0m\n",
            "\u001b[?25hDownloading dash-3.2.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m136.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask-3.1.2-py3-none-any.whl (103 kB)\n",
            "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
            "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
            "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
            "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
            "Downloading ipython-8.37.0-py3-none-any.whl (831 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.9/831.9 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prompt_toolkit-3.0.51-py3-none-any.whl (387 kB)\n",
            "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parso-0.8.5-py2.py3-none-any.whl (106 kB)\n",
            "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
            "Downloading matplotlib-3.10.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m130.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.59.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
            "Downloading fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
            "Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
            "Downloading jupyter_core-5.8.1-py3-none-any.whl (28 kB)\n",
            "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
            "Downloading platformdirs-4.3.8-py3-none-any.whl (18 kB)\n",
            "Downloading plotly-6.3.0-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m132.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading narwhals-2.1.2-py3-none-any.whl (392 kB)\n",
            "Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
            "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
            "Downloading rpds_py-0.27.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n",
            "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
            "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading configargparse-1.7.1-py3-none-any.whl (25 kB)\n",
            "Downloading decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
            "Downloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
            "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
            "Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
            "Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
            "Downloading retrying-1.4.2-py3-none-any.whl (10 kB)\n",
            "Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61445 sha256=86163068771a1dcac62192aabd1baa543b0e58a2ed7b852ab36d3721c00dacbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31600 sha256=f2cd61a113d82c8999337f2d736fc88791eb179baad53d8b986fbf907e4cccf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: wcwidth, pure-eval, ptyprocess, fastjsonschema, addict, zipp, widgetsnbextension, werkzeug, urllib3, typing_extensions, triton, traitlets, threadpoolctl, termcolor, tabulate, rpds-py, retrying, pyyaml, pyparsing, pygments, prompt_toolkit, portalocker, platformdirs, pexpect, parso, packaging, numpy, nest-asyncio, narwhals, kiwisolver, jupyterlab_widgets, joblib, itsdangerous, idna, fonttools, executing, decorator, cycler, configargparse, comm, click, charset_normalizer, certifi, blinker, attrs, asttokens, yacs, trimesh, torch, stack_data, requests, referencing, pyquaternion, plotly, pandas, matplotlib-inline, jupyter-core, jedi, iopath, importlib-metadata, flask, exceptiongroup, contourpy, torchvision, torchaudio, scikit-learn, matplotlib, jsonschema-specifications, ipython, fvcore, dash, pytorch3d, jsonschema, ipywidgets, nbformat, open3d\n",
            "\u001b[2K  Attempting uninstall: typing_extensions\n",
            "\u001b[2K    Found existing installation: typing_extensions 4.14.1\n",
            "\u001b[2K    Uninstalling typing_extensions-4.14.1:\n",
            "\u001b[2K      Successfully uninstalled typing_extensions-4.14.1\n",
            "\u001b[2K  Attempting uninstall: triton\n",
            "\u001b[2K    Found existing installation: triton 2.3.0\n",
            "\u001b[2K    Uninstalling triton-2.3.0:\n",
            "\u001b[2K      Successfully uninstalled triton-2.3.0\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 2.2.6\n",
            "\u001b[2K    Uninstalling numpy-2.2.6:\n",
            "\u001b[2K      Successfully uninstalled numpy-2.2.6\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 2.3.0+cu121\n",
            "\u001b[2K    Uninstalling torch-2.3.0+cu121:\n",
            "\u001b[2K      Successfully uninstalled torch-2.3.0+cu121\n",
            "\u001b[2K  Attempting uninstall: pandas\n",
            "\u001b[2K    Found existing installation: pandas 2.3.2\n",
            "\u001b[2K    Uninstalling pandas-2.3.2:\n",
            "\u001b[2K      Successfully uninstalled pandas-2.3.2\n",
            "\u001b[2K  Attempting uninstall: torchvision\n",
            "\u001b[2K    Found existing installation: torchvision 0.18.0+cu121\n",
            "\u001b[2K    Uninstalling torchvision-0.18.0+cu121:\n",
            "\u001b[2K      Successfully uninstalled torchvision-0.18.0+cu121\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76/76\u001b[0m [open3d]\n",
            "\u001b[1A\u001b[2KSuccessfully installed addict-2.4.0 asttokens-3.0.0 attrs-25.3.0 blinker-1.9.0 certifi-2025.8.3 charset_normalizer-3.4.3 click-8.2.1 comm-0.2.3 configargparse-1.7.1 contourpy-1.3.2 cycler-0.12.1 dash-3.2.0 decorator-5.2.1 exceptiongroup-1.3.0 executing-2.2.0 fastjsonschema-2.21.2 flask-3.1.2 fonttools-4.59.1 fvcore-0.1.5.post20221221 idna-3.10 importlib-metadata-8.7.0 iopath-0.1.10 ipython-8.37.0 ipywidgets-8.1.7 itsdangerous-2.2.0 jedi-0.19.2 joblib-1.5.1 jsonschema-4.25.1 jsonschema-specifications-2025.4.1 jupyter-core-5.8.1 jupyterlab_widgets-3.0.15 kiwisolver-1.4.9 matplotlib-3.10.5 matplotlib-inline-0.1.7 narwhals-2.1.2 nbformat-5.10.4 nest-asyncio-1.6.0 numpy-1.26.4 open3d-0.19.0 packaging-25.0 pandas-1.5.3 parso-0.8.5 pexpect-4.9.0 platformdirs-4.3.8 plotly-6.3.0 portalocker-3.2.0 prompt_toolkit-3.0.51 ptyprocess-0.7.0 pure-eval-0.2.3 pygments-2.19.2 pyparsing-3.2.3 pyquaternion-0.9.9 pytorch3d-0.7.5 pyyaml-6.0.2 referencing-0.36.2 requests-2.32.5 retrying-1.4.2 rpds-py-0.27.0 scikit-learn-1.4.0 stack_data-0.6.3 tabulate-0.9.0 termcolor-3.1.0 threadpoolctl-3.6.0 torch-2.1.0+cu121 torchaudio-2.1.0+cu121 torchvision-0.16.0+cu121 traitlets-5.14.3 trimesh-4.7.4 triton-2.1.0 typing_extensions-4.12.2 urllib3-2.5.0 wcwidth-0.2.13 werkzeug-3.1.3 widgetsnbextension-4.0.14 yacs-0.1.8 zipp-3.23.0\n",
            "Virtual environment setup complete.\n"
          ]
        }
      ],
      "source": [
        "# Set this variable to True to install GPU dependencies, False for CPU\n",
        "use_gpu = True # @param {type:\"boolean\"}\n",
        "\n",
        "# Install dependencies based on the use_gpu flag\n",
        "\n",
        "import os\n",
        "\n",
        "if use_gpu:\n",
        "  print(\"Setting up GPU virtual environment...\")\n",
        "  venv_dir = 'venv_gpu'\n",
        "  requirements_file = 'requirements_gpu.txt'\n",
        "else:\n",
        "  print(\"Setting up CPU virtual environment...\")\n",
        "  venv_dir = 'venv_cpu'\n",
        "  requirements_file = 'requirements_cpu.txt'\n",
        "\n",
        "# Check if the virtual environment directory already exists\n",
        "# !sudo update-alternatives --set python3 /usr/bin/python3.10\n",
        "\n",
        "full_venv_dir = os.path.join(repo_dir, venv_dir)\n",
        "venv_activate = full_venv_dir+ \"/bin/activate\"\n",
        "venv_python = full_venv_dir+ \"/bin/python\"\n",
        "if not os.path.exists(full_venv_dir):\n",
        "  print(f'No virtual environment found at {venv_dir}. Running setup script...')\n",
        "  # Run the setup script\n",
        "  !pip install virtualenv\n",
        "  !virtualenv -p /usr/bin/python3.10 {full_venv_dir}\n",
        "  !source {venv_activate}\n",
        "  !echo \"Activating virtual environment...\"\n",
        "  !echo \"Installing dependencies from {requirements_file}...\"\n",
        "  !echo venv_python={venv_python}\n",
        "\n",
        "  !{venv_python} -m pip install -r {requirements_file}\n",
        "\n",
        "  !echo \"Virtual environment setup complete.\"\n",
        "\n",
        "else:\n",
        "    print(f'Virtual environment already exists at {full_venv_dir}. Skipping setup script.')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8a733dc6",
        "outputId": "01859028-170c-443e-83b9-81ae41a60873"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Define the virtual environment path\n",
        "venv_python = os.path.join(full_venv_dir, 'bin', 'python')\n",
        "venv_site_packages = os.path.join(full_venv_dir, 'lib', 'python3.10', 'site-packages') # Assuming python3.10\n",
        "\n",
        "print(f\"Virtual environment path: {full_venv_dir}\")\n",
        "print(f\"Virtual environment site-packages path: {venv_site_packages}\")\n",
        "print(f\"Virtual environment python executable: {venv_python}\")\n",
        "\n",
        "# Check if the site-packages directory exists and list its contents\n",
        "if os.path.exists(venv_site_packages):\n",
        "    print(f\"\\nListing contents of {venv_site_packages}:\")\n",
        "    # Limit the output to avoid flooding the display if there are many files\n",
        "    contents = os.listdir(venv_site_packages)\n",
        "    print(f\"Found {len(contents)} items.\")\n",
        "    # Print a sample of the contents\n",
        "    for item in contents[:20]: # Print first 20 items\n",
        "        print(item)\n",
        "    if len(contents) > 20:\n",
        "        print(\"...\")\n",
        "else:\n",
        "    print(f\"\\nSite-packages directory not found at {venv_site_packages}\")\n",
        "\n",
        "# Attempt to import the libraries using the virtual environment's python\n",
        "print(\"\\nAttempting to import libraries using venv python:\")\n",
        "import subprocess\n",
        "\n",
        "libraries_to_check = ['numpy', 'open3d', 'pytorch3d', 'torch', 'pyntcloud']\n",
        "\n",
        "for lib in libraries_to_check:\n",
        "    try:\n",
        "        # Use the virtual environment's python to run a command that imports the library\n",
        "        command = [venv_python, '-c', f'import {lib}; print(f\"{lib} imported successfully\")']\n",
        "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
        "        print(result.stdout.strip())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Virtual environment python executable not found at {venv_python}\")\n",
        "        break # Stop if venv python is not found\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"{lib} import failed: {e.stderr.strip()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while checking {lib}: {e}\")"
      ],
      "id": "8a733dc6",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virtual environment path: /content/drive/MyDrive/projects/point-cloud-compression/venv_gpu\n",
            "Virtual environment site-packages path: /content/drive/MyDrive/projects/point-cloud-compression/venv_gpu/lib/python3.10/site-packages\n",
            "Virtual environment python executable: /content/drive/MyDrive/projects/point-cloud-compression/venv_gpu/bin/python\n",
            "\n",
            "Listing contents of /content/drive/MyDrive/projects/point-cloud-compression/venv_gpu/lib/python3.10/site-packages:\n",
            "Found 227 items.\n",
            "_virtualenv.pth\n",
            "_virtualenv.py\n",
            "distutils-precedence.pth\n",
            "pip-25.2.virtualenv\n",
            "setuptools-80.9.0.virtualenv\n",
            "__pycache__\n",
            "pytz\n",
            "pytz-2025.2.dist-info\n",
            "mpmath\n",
            "mpmath-1.3.0.dist-info\n",
            "tzdata\n",
            "tzdata-2025.2.dist-info\n",
            "typing_extensions.py\n",
            "tqdm\n",
            "tqdm-4.67.1.dist-info\n",
            "torchac\n",
            "torchac-0.9.3.dist-info\n",
            "isympy.py\n",
            "sympy\n",
            "sympy-1.14.0.dist-info\n",
            "...\n",
            "\n",
            "Attempting to import libraries using venv python:\n",
            "numpy imported successfully\n",
            "open3d imported successfully\n",
            "pytorch3d imported successfully\n",
            "torch imported successfully\n",
            "pyntcloud imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84cd86f4",
      "metadata": {
        "id": "84cd86f4"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "This step involves downloading and preparing the datasets required for the point cloud compression project. Specifically, it downloads the pre-converted ModelNet40 and ShapeNet point cloud datasets from Google Drive if they are not already present in the `data` directory, and extracts the contents of the downloaded zip files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f0d2ee8",
      "metadata": {
        "cellView": "code",
        "collapsed": true,
        "id": "0f0d2ee8"
      },
      "outputs": [],
      "source": [
        "# Download ModelNet40 and ShapeNet datasets from Google Drive (Option 1 from README)\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "def download_from_gdrive(url, output_path):\n",
        "    try:\n",
        "        import gdown\n",
        "    except ImportError:\n",
        "        os.system('pip install gdown')\n",
        "        import gdown\n",
        "    gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "# Install rarfile for extracting .rar files\n",
        "try:\n",
        "    import rarfile\n",
        "except ImportError:\n",
        "    print(\"rarfile not found, installing...\")\n",
        "    os.system('pip install rarfile')\n",
        "    import rarfile\n",
        "\n",
        "# ModelNet40\n",
        "repo_root = os.getcwd()\n",
        "modelnet_url = 'https://drive.google.com/uc?id=1Isa8seckZ9oNzstlE7VZcd6wVVx8LdMF'\n",
        "# Assuming the file is actually a .rar despite the variable name\n",
        "modelnet_archive = 'ModelNet40_pc_8192.rar'\n",
        "if not os.path.exists(repo_root + '/data/ModelNet40_pc_01_8192p'):\n",
        "    print('Downloading ModelNet40 pre-converted point clouds...')\n",
        "    download_from_gdrive(modelnet_url, modelnet_archive)\n",
        "    try:\n",
        "        with rarfile.RarFile(modelnet_archive, 'r') as rf:\n",
        "            rf.extractall('data')\n",
        "        os.remove(modelnet_archive)\n",
        "        print('ModelNet40 download and extraction complete.')\n",
        "    except rarfile.BadRarFile as e:\n",
        "        print(f\"Error extracting ModelNet40: {e}\")\n",
        "        print(\"Please ensure the downloaded file is a valid .rar archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during ModelNet40 extraction: {e}\")\n",
        "\n",
        "else:\n",
        "    print('ModelNet40 dataset already exists. Skipping download.')\n",
        "\n",
        "# ShapeNet\n",
        "shapenet_url = 'https://drive.google.com/uc?id=1OzaU01kolBpfRRD0zKESYh67Hh2s2dbD'\n",
        "# Assuming the file is actually a .rar despite the variable name\n",
        "shapenet_archive = 'ShapeNet_pc_2048.rar'\n",
        "if not os.path.exists(repo_root + '/data/ShapeNet_pc_01_2048p'):\n",
        "    print('Downloading ShapeNet pre-converted point clouds...')\n",
        "    download_from_gdrive(shapenet_url, shapenet_archive)\n",
        "    try:\n",
        "        with rarfile.RarFile(shapenet_archive, 'r') as rf:\n",
        "            rf.extractall('data')\n",
        "        os.remove(shapenet_archive)\n",
        "        print('ShapeNet download and extraction complete.')\n",
        "    except rarfile.BadRarFile as e:\n",
        "        print(f\"Error extracting ShapeNet: {e}\")\n",
        "        print(\"Please ensure the downloaded file is a valid .rar archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during ShapeNet extraction: {e}\")\n",
        "else:\n",
        "    print('ShapeNet dataset already exists. Skipping download.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc735bd8",
      "metadata": {
        "id": "dc735bd8"
      },
      "source": [
        "## Repository Scripts Overview\n",
        "\n",
        "This section describes the core scripts from the `point-cloud-compression` repository that are executed in this notebook to demonstrate the project's functionality:\n",
        "\n",
        "\n",
        "1.  **`train.py`**: This script trains the autoencoder model on the ModelNet40 training set. It takes input point cloud file paths, an output directory for the trained model, and the patch size (K) as arguments.\n",
        "2.  **`compress.py`**: This script is used to compress point cloud files. It takes input point cloud file paths, an output directory for compressed files, and a trained model path as arguments.\n",
        "3.  **`decompress.py`**: This script performs the decompression of the compressed point cloud files generated by `compress.py`. It requires the directory containing compressed files, an output directory for decompressed files, and the trained model path.\n",
        "4.  **`eval.py`**: This script evaluates the performance of the compression and decompression process. It compares the original point clouds with the decompressed ones using metrics such as PSNR (Peak Signal-to-Noise Ratio), Chamfer distance, and bits per point (bpp). The evaluation results are typically saved to a CSV file.\n",
        "5.  **`visualize.py`**: This script is used to visualize the evaluation metrics generated by `eval.py`. It reads the results from the CSV file and generates plots or figures to help analyze the compression performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f4388e25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f4388e25",
        "outputId": "85078d94-c571-4747-8e44-490dadcbdec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in 'data': ['ShapeNet_pc_01_2048p', 'ModelNet40_pc_01_8192p', '.gitkeep']\n"
          ]
        }
      ],
      "source": [
        "# Example core functionality: List files in the data directory\n",
        "\n",
        "def list_data_files(data_dir='data'):\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Directory '{data_dir}' does not exist.\")\n",
        "        return []\n",
        "    files = os.listdir(data_dir)\n",
        "    print(f\"Files in '{data_dir}':\", files)\n",
        "    return files\n",
        "\n",
        "# Run the function\n",
        "data_files = list_data_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b929b9",
      "metadata": {
        "collapsed": true,
        "id": "a6b929b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0285e45b-b75d-48ff-89e2-12f825fcab0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing on device (gpu/cpu): cuda\n",
            "loading point clouds...\n",
            " 32% 3137/9843 [07:42<19:10,  5.83it/s]"
          ]
        }
      ],
      "source": [
        "# Train the autoencoder model on the ModelNet40 training set\n",
        "# Use the virtual environment's python to run the script\n",
        "!{venv_python} train.py  --train_glob './data/ModelNet40_pc_01_8192p/**/train/*.ply' --model_save_folder './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33385f34",
      "metadata": {
        "id": "33385f34"
      },
      "outputs": [],
      "source": [
        "# Compress point cloud test files using the trained model\n",
        "!{venv_python} compress.py './data/ModelNet40_pc_01_8192p/**/test/*.ply' './data/ModelNet40_K256_compressed' './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "406dce46",
      "metadata": {
        "id": "406dce46"
      },
      "outputs": [],
      "source": [
        "# Decompress the compressed point cloud files\n",
        "!{venv_python} decompress.py './data/ModelNet40_K256_compressed' './data/ModelNet40_K256_decompressed' './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec7f1ca",
      "metadata": {
        "id": "1ec7f1ca"
      },
      "outputs": [],
      "source": [
        "# Evaluate the compression results using PSNR, Chamfer distance, and bpp metrics\n",
        "!{venv_python} eval.py --input_glob './data/ModelNet40_pc_01_8192p/**/test/*.ply' --compressed_path './data/ModelNet40_K256_compressed' --decompressed_path './data/ModelNet40_K256_decompressed' --output_file './eval/ModelNet40_K256.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06630165",
      "metadata": {
        "id": "06630165"
      },
      "outputs": [],
      "source": [
        "# Visualize evaluation metrics and save plots\n",
        "!{venv_python} visualize.py --csv './eval/ModelNet40_K256.csv' --outdir './figure/'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}