{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhmes/point-cloud-compression/blob/main/point_cloud_compression_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4161fdd0",
      "metadata": {
        "id": "4161fdd0"
      },
      "source": [
        "# Point Cloud Compression Demo\n",
        "\n",
        "This notebook demonstrates installation, core functionality, and testing for the point cloud compression project."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a81a4a6b",
      "metadata": {
        "id": "a81a4a6b"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "This section prepares the environment for the point cloud compression project:\n",
        "\n",
        "1.  **Mount Google Drive**: Connects Colab to Google Drive for persistent storage of the repository and virtual environment.\n",
        "2.  **Clone/Update Repository**: Clones the project repository from GitHub to Google Drive or updates it if it already exists.\n",
        "3.  **Virtual Environment Setup**: Creates and populates a virtual environment (`venv_gpu` or `venv_cpu`) in Google Drive using a setup script, only if it doesn't exist.\n",
        "4.  **Prepare Environment**: Modifies Python's `sys.path` to include the virtual environment's packages, allowing the notebook to use installed dependencies across sessions.\n",
        "5.  **Verify Dependencies**: Checks if key libraries from the virtual environment can be imported successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "daa1006f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "daa1006f",
        "outputId": "ac07c6c2-3922-434a-9acc-1ff84e154f67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted.\n"
          ]
        }
      ],
      "source": [
        "# Connect to Google Drive (for Colab users)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print('Google Drive mounted.')\n",
        "except ImportError:\n",
        "    print('Not running in Colab, skipping Google Drive mount.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a48bcdf3",
      "metadata": {
        "collapsed": true,
        "id": "a48bcdf3",
        "outputId": "e6057626-6d7a-4e47-cafd-4d0129b95389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating point-cloud-compression...\n",
            "Already up to date.\n",
            "Current working directory: /content/drive/MyDrive/projects/point-cloud-compression\n"
          ]
        }
      ],
      "source": [
        "# Clone repo only if not already cloned\n",
        "import os\n",
        "repo_url = 'https://github.com/rhmes/point-cloud-compression.git'\n",
        "parent_dir = '/content/drive/MyDrive/projects'\n",
        "repo_dir = os.path.join(parent_dir, 'point-cloud-compression')\n",
        "os.makedirs(parent_dir, exist_ok=True)\n",
        "os.chdir(parent_dir)\n",
        "if not os.path.exists(repo_dir):\n",
        "  print('Cloning point-cloud-compression...')\n",
        "  !git clone {repo_url}\n",
        "else:\n",
        "  print('Updating point-cloud-compression...')\n",
        "  !cd {repo_dir} && git pull\n",
        "os.chdir(repo_dir)\n",
        "print(f'Current working directory: {os.getcwd()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1a3fd280",
      "metadata": {
        "collapsed": true,
        "id": "1a3fd280",
        "outputId": "395c5e80-33a2-4f8b-be71-1e5c5122b076",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up GPU virtual environment...\n",
            "Virtual environment already exists at /content/drive/MyDrive/projects/point-cloud-compression/venv_gpu. Skipping setup script.\n"
          ]
        }
      ],
      "source": [
        "# Set this variable to True to install GPU dependencies, False for CPU\n",
        "use_gpu = True # @param {type:\"boolean\"}\n",
        "\n",
        "# Install dependencies based on the use_gpu flag\n",
        "\n",
        "import os\n",
        "\n",
        "if use_gpu:\n",
        "  print(\"Setting up GPU virtual environment...\")\n",
        "  venv_dir = 'venv_gpu'\n",
        "  requirements_file = 'requirements_gpu.txt'\n",
        "else:\n",
        "  print(\"Setting up CPU virtual environment...\")\n",
        "  venv_dir = 'venv_cpu'\n",
        "  requirements_file = 'requirements_cpu.txt'\n",
        "\n",
        "# Check if the virtual environment directory already exists\n",
        "full_venv_dir = os.path.join(repo_dir, venv_dir)\n",
        "venv_activate = full_venv_dir+ \"/bin/activate\"\n",
        "venv_python = full_venv_dir+ \"/bin/python\"\n",
        "if not os.path.exists(full_venv_dir):\n",
        "  print(f'No virtual environment found at {venv_dir}. Running setup script...')\n",
        "  # Run the setup script\n",
        "  !pip install virtualenv\n",
        "  !virtualenv {full_venv_dir}\n",
        "  !source {venv_activate}\n",
        "  !echo \"Activating virtual environment...\"\n",
        "  !echo \"Installing dependencies from {requirements_file}...\"\n",
        "  !echo venv_python={venv_python}\n",
        "  !{venv_python} -m pip install -r {requirements_file}\n",
        "\n",
        "  !{venv_python} -m pip install --upgrade pip setuptools wheel\n",
        "  # Addintional libararies insatll\n",
        "  !{venv_python} -m pip install \"git+https://github.com/facebookresearch/pytorch3d.git\"\n",
        "  !echo \"Virtual environment setup complete.\"\n",
        "\n",
        "  # Addintionl lib\n",
        "\n",
        "else:\n",
        "    print(f'Virtual environment already exists at {full_venv_dir}. Skipping setup script.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5b3b69d2",
      "metadata": {
        "collapsed": true,
        "id": "5b3b69d2",
        "outputId": "ff3f3ae3-2e38-4c6a-9516-3c35a8cadff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/projects/point-cloud-compression/venv_gpu/lib/python3.11/site-packages already in sys.path\n",
            "Successfully imported torch (version: 2.3.0+cu121)\n",
            "Successfully imported open3d\n",
            "Successfully imported pytorch3d\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Prepare the virtual environment's site-packages\n",
        "venv_path = full_venv_dir\n",
        "\n",
        "# Common site-packages paths relative to the venv directory\n",
        "site_packages_paths_candidates = [\n",
        "  os.path.join(venv_path, 'lib', f'python{sys.version_info.major}.{sys.version_info.minor}', 'site-packages'),\n",
        "  os.path.join(venv_path, 'lib64', f'python{sys.version_info.major}.{sys.version_info.minor}', 'site-packages'),\n",
        "]\n",
        "\n",
        "site_packages_path = None\n",
        "for candidate in site_packages_paths_candidates:\n",
        "  if os.path.exists(candidate):\n",
        "    site_packages_path = candidate\n",
        "    break\n",
        "\n",
        "if site_packages_path and site_packages_path not in sys.path:\n",
        "  sys.path.insert(0, site_packages_path)\n",
        "  print(f\"Added {site_packages_path} to sys.path\")\n",
        "elif site_packages_path:\n",
        "  print(f\"{site_packages_path} already in sys.path\")\n",
        "else:\n",
        "  print(f\"Could not find site-packages directory in {venv_path}\")\n",
        "\n",
        "# Verify that packages from the venv can be imported (optional)\n",
        "try:\n",
        "  import torch\n",
        "  print(f\"Successfully imported torch (version: {torch.__version__})\")\n",
        "except ImportError:\n",
        "  print(\"Could not import torch. Ensure virtual environment is set up correctly.\")\n",
        "\n",
        "try:\n",
        "  import open3d\n",
        "  print(\"Successfully imported open3d\")\n",
        "except ImportError:\n",
        "  print(\"Could not import open3d. Ensure virtual environment is set up correctly.\")\n",
        "\n",
        "try:\n",
        "  import pytorch3d\n",
        "  from pytorch3d.ops.knn import _KNN, knn_gather, knn_points\n",
        "\n",
        "  print(\"Successfully imported pytorch3d\")\n",
        "except ImportError:\n",
        "  print(\"Could not import pytorch3d. Ensure virtual environment is set up correctly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84cd86f4",
      "metadata": {
        "id": "84cd86f4"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "This step involves downloading and preparing the datasets required for the point cloud compression project. Specifically, it downloads the pre-converted ModelNet40 and ShapeNet point cloud datasets from Google Drive if they are not already present in the `data` directory, and extracts the contents of the downloaded zip files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0f0d2ee8",
      "metadata": {
        "cellView": "code",
        "collapsed": true,
        "id": "0f0d2ee8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ffe07e1-d932-4f7b-dd2d-4c379efb45ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelNet40 dataset already exists. Skipping download.\n",
            "ShapeNet dataset already exists. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "# Download ModelNet40 and ShapeNet datasets from Google Drive (Option 1 from README)\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "def download_from_gdrive(url, output_path):\n",
        "    try:\n",
        "        import gdown\n",
        "    except ImportError:\n",
        "        os.system('pip install gdown')\n",
        "        import gdown\n",
        "    gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "# Install rarfile for extracting .rar files\n",
        "try:\n",
        "    import rarfile\n",
        "except ImportError:\n",
        "    print(\"rarfile not found, installing...\")\n",
        "    os.system('pip install rarfile')\n",
        "    import rarfile\n",
        "\n",
        "# ModelNet40\n",
        "repo_root = os.getcwd()\n",
        "modelnet_url = 'https://drive.google.com/uc?id=1Isa8seckZ9oNzstlE7VZcd6wVVx8LdMF'\n",
        "# Assuming the file is actually a .rar despite the variable name\n",
        "modelnet_archive = 'ModelNet40_pc_8192.rar'\n",
        "if not os.path.exists(repo_root + '/data/ModelNet40_pc_01_8192p'):\n",
        "    print('Downloading ModelNet40 pre-converted point clouds...')\n",
        "    download_from_gdrive(modelnet_url, modelnet_archive)\n",
        "    try:\n",
        "        with rarfile.RarFile(modelnet_archive, 'r') as rf:\n",
        "            rf.extractall('data')\n",
        "        os.remove(modelnet_archive)\n",
        "        print('ModelNet40 download and extraction complete.')\n",
        "    except rarfile.BadRarFile as e:\n",
        "        print(f\"Error extracting ModelNet40: {e}\")\n",
        "        print(\"Please ensure the downloaded file is a valid .rar archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during ModelNet40 extraction: {e}\")\n",
        "\n",
        "else:\n",
        "    print('ModelNet40 dataset already exists. Skipping download.')\n",
        "\n",
        "# ShapeNet\n",
        "shapenet_url = 'https://drive.google.com/uc?id=1OzaU01kolBpfRRD0zKESYh67Hh2s2dbD'\n",
        "# Assuming the file is actually a .rar despite the variable name\n",
        "shapenet_archive = 'ShapeNet_pc_2048.rar'\n",
        "if not os.path.exists(repo_root + '/data/ShapeNet_pc_01_2048p'):\n",
        "    print('Downloading ShapeNet pre-converted point clouds...')\n",
        "    download_from_gdrive(shapenet_url, shapenet_archive)\n",
        "    try:\n",
        "        with rarfile.RarFile(shapenet_archive, 'r') as rf:\n",
        "            rf.extractall('data')\n",
        "        os.remove(shapenet_archive)\n",
        "        print('ShapeNet download and extraction complete.')\n",
        "    except rarfile.BadRarFile as e:\n",
        "        print(f\"Error extracting ShapeNet: {e}\")\n",
        "        print(\"Please ensure the downloaded file is a valid .rar archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during ShapeNet extraction: {e}\")\n",
        "else:\n",
        "    print('ShapeNet dataset already exists. Skipping download.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc735bd8",
      "metadata": {
        "id": "dc735bd8"
      },
      "source": [
        "## Repository Scripts Overview\n",
        "\n",
        "This section describes the core scripts from the `point-cloud-compression` repository that are executed in this notebook to demonstrate the project's functionality:\n",
        "\n",
        "\n",
        "1.  **`train.py`**: This script trains the autoencoder model on the ModelNet40 training set. It takes input point cloud file paths, an output directory for the trained model, and the patch size (K) as arguments.\n",
        "2.  **`compress.py`**: This script is used to compress point cloud files. It takes input point cloud file paths, an output directory for compressed files, and a trained model path as arguments.\n",
        "3.  **`decompress.py`**: This script performs the decompression of the compressed point cloud files generated by `compress.py`. It requires the directory containing compressed files, an output directory for decompressed files, and the trained model path.\n",
        "4.  **`eval.py`**: This script evaluates the performance of the compression and decompression process. It compares the original point clouds with the decompressed ones using metrics such as PSNR (Peak Signal-to-Noise Ratio), Chamfer distance, and bits per point (bpp). The evaluation results are typically saved to a CSV file.\n",
        "5.  **`visualize.py`**: This script is used to visualize the evaluation metrics generated by `eval.py`. It reads the results from the CSV file and generates plots or figures to help analyze the compression performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f4388e25",
      "metadata": {
        "collapsed": true,
        "id": "f4388e25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f0399fd-5ff8-4016-8d07-da3a6f1daa7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in 'data': ['ShapeNet_pc_01_2048p', 'ModelNet40_pc_01_8192p', '.gitkeep']\n"
          ]
        }
      ],
      "source": [
        "# Example core functionality: List files in the data directory\n",
        "\n",
        "def list_data_files(data_dir='data'):\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Directory '{data_dir}' does not exist.\")\n",
        "        return []\n",
        "    files = os.listdir(data_dir)\n",
        "    print(f\"Files in '{data_dir}':\", files)\n",
        "    return files\n",
        "\n",
        "# Run the function\n",
        "data_files = list_data_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b929b9",
      "metadata": {
        "collapsed": true,
        "id": "a6b929b9"
      },
      "outputs": [],
      "source": [
        "# Train the autoencoder model on the ModelNet40 training set\n",
        "!{venv_python} train.py './data/ModelNet40_pc_01_8192p/**/train/*.ply' './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33385f34",
      "metadata": {
        "id": "33385f34"
      },
      "outputs": [],
      "source": [
        "# Compress point cloud test files using the trained model\n",
        "!{venv_python} compress.py './data/ModelNet40_pc_01_8192p/**/test/*.ply' './data/ModelNet40_K256_compressed' './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "406dce46",
      "metadata": {
        "id": "406dce46"
      },
      "outputs": [],
      "source": [
        "# Decompress the compressed point cloud files\n",
        "!{venv_python} decompress.py './data/ModelNet40_K256_compressed' './data/ModelNet40_K256_decompressed' './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec7f1ca",
      "metadata": {
        "id": "1ec7f1ca"
      },
      "outputs": [],
      "source": [
        "# Evaluate the compression results using PSNR, Chamfer distance, and bpp metrics\n",
        "!{venv_python} eval.py './data/ModelNet40_pc_01_8192p/**/test/*.ply' './data/ModelNet40_K256_compressed' './data/ModelNet40_K256_decompressed' './eval/ModelNet40_K256.csv' '../geo_dist/build/pc_error'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06630165",
      "metadata": {
        "id": "06630165"
      },
      "outputs": [],
      "source": [
        "# Visualize evaluation metrics and save plots\n",
        "!{venv_python} visualize.py --csv './eval/ModelNet40_K256.csv' --outdir './figure/'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}