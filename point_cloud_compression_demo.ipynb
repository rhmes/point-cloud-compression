{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhmes/point-cloud-compression/blob/main/point_cloud_compression_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4161fdd0",
      "metadata": {
        "id": "4161fdd0"
      },
      "source": [
        "# Point Cloud Compression Demo\n",
        "\n",
        "This notebook demonstrates installation, core functionality, and testing for the point cloud compression project."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a81a4a6b",
      "metadata": {
        "id": "a81a4a6b"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "This section prepares the environment for the point cloud compression project:\n",
        "\n",
        "1.  **Mount Google Drive**: Connects Colab to Google Drive for persistent storage of the repository and virtual environment.\n",
        "2.  **Clone/Update Repository**: Clones the project repository from GitHub to Google Drive or updates it if it already exists.\n",
        "3.  **Virtual Environment Setup**: Creates and populates a virtual environment (`venv_gpu` or `venv_cpu`) in Google Drive using a setup script, only if it doesn't exist.\n",
        "4.  **Prepare Environment**: Modifies Python's `sys.path` to include the virtual environment's packages, allowing the notebook to use installed dependencies across sessions.\n",
        "5.  **Verify Dependencies**: Checks if key libraries from the virtual environment can be imported successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "daa1006f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "daa1006f",
        "outputId": "6237d129-c3d2-4e1f-bb60-edda90098a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted.\n"
          ]
        }
      ],
      "source": [
        "# Connect to Google Drive (for Colab users)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print('Google Drive mounted.')\n",
        "except ImportError:\n",
        "    print('Not running in Colab, skipping Google Drive mount.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a48bcdf3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a48bcdf3",
        "outputId": "b68c6fee-9eae-4333-f977-00be8632dbd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating point-cloud-compression...\n",
            "Already up to date.\n",
            "Current working directory: /content/drive/MyDrive/projects/point-cloud-compression\n"
          ]
        }
      ],
      "source": [
        "# Clone repo only if not already cloned\n",
        "import os\n",
        "repo_url = 'https://github.com/rhmes/point-cloud-compression.git'\n",
        "parent_dir = '/content/drive/MyDrive/projects'\n",
        "repo_dir = os.path.join(parent_dir, 'point-cloud-compression')\n",
        "os.makedirs(parent_dir, exist_ok=True)\n",
        "os.chdir(parent_dir)\n",
        "if not os.path.exists(repo_dir):\n",
        "  print('Cloning point-cloud-compression...')\n",
        "  !git clone {repo_url}\n",
        "else:\n",
        "  print('Updating point-cloud-compression...')\n",
        "  !cd {repo_dir} && git pull\n",
        "os.chdir(repo_dir)\n",
        "print(f'Current working directory: {os.getcwd()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3fd280",
      "metadata": {
        "collapsed": true,
        "id": "1a3fd280"
      },
      "outputs": [],
      "source": [
        "# Set this variable to True to set up GPU virtual environment, False for CPU\n",
        "use_gpu = True # @param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "# Determine the virtual environment directory and requirements file\n",
        "if use_gpu:\n",
        "    print(\"Checking for GPU virtual environment...\")\n",
        "    venv_dir = 'venv_gpu'\n",
        "    requirements_file = 'requirements_gpu.txt'\n",
        "else:\n",
        "    print(\"Checking for CPU virtual environment...\")\n",
        "    venv_dir = 'venv_cpu'\n",
        "    requirements_file = 'requirements_cpu.txt'\n",
        "\n",
        "\n",
        "# Construct the full path to the virtual environment directory in Google Drive\n",
        "repo_root = os.getcwd()\n",
        "full_venv_path = os.path.join(repo_root, venv_dir)\n",
        "\n",
        "\n",
        "# Check if the virtual environment directory already exists in Google Drive\n",
        "if os.path.exists(full_venv_path):\n",
        "    print(f'Virtual environment already exists at {full_venv_path}.\\nSkipping dependency installation.')\n",
        "else:\n",
        "    print(f'Virtual environment not found at {full_venv_path}. Proceeding with setup.')\n",
        "\n",
        "    # Create a marker file to indicate that setup has been run\n",
        "    os.makedirs(full_venv_path, exist_ok=True)\n",
        "    setup_complete_marker = os.path.join(full_venv_path, '.setup_complete')\n",
        "    if not os.path.exists(setup_complete_marker):\n",
        "        print(f\"Installing dependencies from {requirements_file}...\")\n",
        "        !pip install -r {requirements_file}\n",
        "        print(\"Installing additional dependencies...\")\n",
        "        # Install Open3D\n",
        "        !pip install open3d\n",
        "        # Install PyTorch3D from source\n",
        "        !pip install \"git+https://github.com/facebookresearch/pytorch3d.git\"\n",
        "        print(\"Dependency installation complete.\")\n",
        "        # Create the marker file\n",
        "        with open(setup_complete_marker, 'w') as f:\n",
        "            f.write('Setup complete')\n",
        "        print(\"Setup complete marker created.\")\n",
        "    else:\n",
        "        print(\"Setup complete marker found. Skipping dependency installation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b3b69d2",
      "metadata": {
        "collapsed": true,
        "id": "5b3b69d2"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the installation target to sys.path\n",
        "# Use the venv_dir variable determined in the environment setup\n",
        "# Make sure setup cell has been run (at least) once.\n",
        "\n",
        "try:\n",
        "    venv_dir_name = venv_dir\n",
        "except NameError:\n",
        "    print(\"venv_dir variable not found. Please run the environment setup cell first.\")\n",
        "    # Default to 'venv_gpu/venv_cpu' if venv_dir is not defined\n",
        "    use_gpu = True # Assuming GPU setup if variable is not found\n",
        "    if use_gpu:\n",
        "        venv_dir_name = 'venv_gpu'\n",
        "    else:\n",
        "        venv_dir_name = 'venv_cpu'\n",
        "    print(f\"Using default venv_dir_name: {venv_dir_name}. Consider running cell 1a3fd280.\")\n",
        "\n",
        "\n",
        "# Construct the full path to the virtual environment directory in Google Drive\n",
        "repo_root = os.getcwd() # This assumes you are in the repo directory after cloning/updating\n",
        "full_venv_path = os.path.join(repo_root, venv_dir_name)\n",
        "\n",
        "\n",
        "# Add the full_venv_path to sys.path so Python can find the installed packages\n",
        "if full_venv_path not in sys.path:\n",
        "    sys.path.insert(0, full_venv_path)\n",
        "    print(f\"Added {full_venv_path} to sys.path\")\n",
        "else:\n",
        "     print(f\"{full_venv_path} already in sys.path\")\n",
        "\n",
        "\n",
        "# Verify that packages from the venv can be imported (optional)\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"Successfully imported torch (version: {torch.__version__})\")\n",
        "except ImportError:\n",
        "    print(\"Could not import torch. Ensure virtual environment is set up correctly.\")\n",
        "\n",
        "try:\n",
        "    import open3d\n",
        "    print(\"Successfully imported open3d\")\n",
        "except ImportError:\n",
        "    print(\"Could not import open3d. Ensure virtual environment is set up correctly.\")\n",
        "\n",
        "try:\n",
        "    import pytorch3d\n",
        "    print(\"Successfully imported pytorch3d\")\n",
        "except ImportError:\n",
        "    print(\"Could not import pytorch3d. Ensure virtual environment is set up correctly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84cd86f4",
      "metadata": {
        "id": "84cd86f4"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "This step involves downloading and preparing the datasets required for the point cloud compression project. Specifically, it downloads the pre-converted ModelNet40 and ShapeNet point cloud datasets from Google Drive if they are not already present in the `data` directory, and extracts the contents of the downloaded zip files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f0d2ee8",
      "metadata": {
        "cellView": "code",
        "collapsed": true,
        "id": "0f0d2ee8"
      },
      "outputs": [],
      "source": [
        "# Download ModelNet40 and ShapeNet datasets from Google Drive (Option 1 from README)\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "def download_from_gdrive(url, output_path):\n",
        "    try:\n",
        "        import gdown\n",
        "    except ImportError:\n",
        "        os.system('pip install gdown')\n",
        "        import gdown\n",
        "    gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "# Install rarfile for extracting .rar files\n",
        "try:\n",
        "    import rarfile\n",
        "except ImportError:\n",
        "    print(\"rarfile not found, installing...\")\n",
        "    os.system('pip install rarfile')\n",
        "    import rarfile\n",
        "\n",
        "# ModelNet40\n",
        "repo_root = os.getcwd()\n",
        "modelnet_url = 'https://drive.google.com/uc?id=1Isa8seckZ9oNzstlE7VZcd6wVVx8LdMF'\n",
        "# Assuming the file is actually a .rar despite the variable name\n",
        "modelnet_archive = 'ModelNet40_pc_8192.rar'\n",
        "if not os.path.exists(repo_root + '/data/ModelNet40_pc_01_8192p'):\n",
        "    print('Downloading ModelNet40 pre-converted point clouds...')\n",
        "    download_from_gdrive(modelnet_url, modelnet_archive)\n",
        "    try:\n",
        "        with rarfile.RarFile(modelnet_archive, 'r') as rf:\n",
        "            rf.extractall('data')\n",
        "        os.remove(modelnet_archive)\n",
        "        print('ModelNet40 download and extraction complete.')\n",
        "    except rarfile.BadRarFile as e:\n",
        "        print(f\"Error extracting ModelNet40: {e}\")\n",
        "        print(\"Please ensure the downloaded file is a valid .rar archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during ModelNet40 extraction: {e}\")\n",
        "\n",
        "else:\n",
        "    print('ModelNet40 dataset already exists. Skipping download.')\n",
        "\n",
        "# ShapeNet\n",
        "shapenet_url = 'https://drive.google.com/uc?id=1OzaU01kolBpfRRD0zKESYh67Hh2s2dbD'\n",
        "# Assuming the file is actually a .rar despite the variable name\n",
        "shapenet_archive = 'ShapeNet_pc_2048.rar'\n",
        "if not os.path.exists(repo_root + '/data/ShapeNet_pc_01_2048p'):\n",
        "    print('Downloading ShapeNet pre-converted point clouds...')\n",
        "    download_from_gdrive(shapenet_url, shapenet_archive)\n",
        "    try:\n",
        "        with rarfile.RarFile(shapenet_archive, 'r') as rf:\n",
        "            rf.extractall('data')\n",
        "        os.remove(shapenet_archive)\n",
        "        print('ShapeNet download and extraction complete.')\n",
        "    except rarfile.BadRarFile as e:\n",
        "        print(f\"Error extracting ShapeNet: {e}\")\n",
        "        print(\"Please ensure the downloaded file is a valid .rar archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during ShapeNet extraction: {e}\")\n",
        "else:\n",
        "    print('ShapeNet dataset already exists. Skipping download.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc735bd8",
      "metadata": {
        "id": "dc735bd8"
      },
      "source": [
        "## Repository Scripts Overview\n",
        "\n",
        "This section describes the core scripts from the `point-cloud-compression` repository that are executed in this notebook to demonstrate the project's functionality:\n",
        "\n",
        "\n",
        "1.  **`train.py`**: This script trains the autoencoder model on the ModelNet40 training set. It takes input point cloud file paths, an output directory for the trained model, and the patch size (K) as arguments.\n",
        "2.  **`compress.py`**: This script is used to compress point cloud files. It takes input point cloud file paths, an output directory for compressed files, and a trained model path as arguments.\n",
        "3.  **`decompress.py`**: This script performs the decompression of the compressed point cloud files generated by `compress.py`. It requires the directory containing compressed files, an output directory for decompressed files, and the trained model path.\n",
        "4.  **`eval.py`**: This script evaluates the performance of the compression and decompression process. It compares the original point clouds with the decompressed ones using metrics such as PSNR (Peak Signal-to-Noise Ratio), Chamfer distance, and bits per point (bpp). The evaluation results are typically saved to a CSV file.\n",
        "5.  **`visualize.py`**: This script is used to visualize the evaluation metrics generated by `eval.py`. It reads the results from the CSV file and generates plots or figures to help analyze the compression performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4388e25",
      "metadata": {
        "collapsed": true,
        "id": "f4388e25"
      },
      "outputs": [],
      "source": [
        "# Example core functionality: List files in the data directory\n",
        "\n",
        "def list_data_files(data_dir='data'):\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Directory '{data_dir}' does not exist.\")\n",
        "        return []\n",
        "    files = os.listdir(data_dir)\n",
        "    print(f\"Files in '{data_dir}':\", files)\n",
        "    return files\n",
        "\n",
        "# Run the function\n",
        "data_files = list_data_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b929b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a6b929b9",
        "outputId": "5d8bb470-db0f-43f5-9c88-1c06c982d4f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing on device (gpu/cpu): cpu\n",
            "loading point clouds...\n",
            " 34% 3361/9843 [09:20<27:53,  3.87it/s]"
          ]
        }
      ],
      "source": [
        "# Train the autoencoder model on the ModelNet40 training set\n",
        "!python train.py './data/ModelNet40_pc_01_8192p/**/train/*.ply' './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33385f34",
      "metadata": {
        "id": "33385f34"
      },
      "outputs": [],
      "source": [
        "# Compress point cloud test files using the trained model\n",
        "!python compress.py './data/ModelNet40_pc_01_8192p/**/test/*.ply' './data/ModelNet40_K256_compressed' './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "406dce46",
      "metadata": {
        "id": "406dce46"
      },
      "outputs": [],
      "source": [
        "# Decompress the compressed point cloud files\n",
        "!python decompress.py './data/ModelNet40_K256_compressed' './data/ModelNet40_K256_decompressed' './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec7f1ca",
      "metadata": {
        "id": "1ec7f1ca"
      },
      "outputs": [],
      "source": [
        "# Evaluate the compression results using PSNR, Chamfer distance, and bpp metrics\n",
        "!python eval.py './data/ModelNet40_pc_01_8192p/**/test/*.ply' './data/ModelNet40_K256_compressed' './data/ModelNet40_K256_decompressed' './eval/ModelNet40_K256.csv' '../geo_dist/build/pc_error'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06630165",
      "metadata": {
        "id": "06630165"
      },
      "outputs": [],
      "source": [
        "# Visualize evaluation metrics and save plots\n",
        "!python visualize.py --csv './eval/ModelNet40_K256.csv' --outdir './figure/'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}