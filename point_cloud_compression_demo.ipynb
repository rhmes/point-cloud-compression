{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhmes/point-cloud-compression/blob/main/point_cloud_compression_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4161fdd0",
      "metadata": {
        "id": "4161fdd0"
      },
      "source": [
        "# Point Cloud Compression Demo\n",
        "\n",
        "This notebook demonstrates installation, core functionality, and testing for the point cloud compression project."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a81a4a6b",
      "metadata": {
        "id": "a81a4a6b"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "This section prepares the environment for the point cloud compression project:\n",
        "\n",
        "1.  **Mount Google Drive**: Connects Colab to Google Drive for persistent storage of the repository and virtual environment.\n",
        "2.  **Clone/Update Repository**: Clones the project repository from GitHub to Google Drive or updates it if it already exists.\n",
        "3.  **Virtual Environment Setup**: Creates and populates a virtual environment (`venv_gpu` or `venv_cpu`) in Google Drive using a setup script, only if it doesn't exist.\n",
        "4.  **Prepare Environment**: Modifies Python's `sys.path` to include the virtual environment's packages, allowing the notebook to use installed dependencies across sessions.\n",
        "5.  **Verify Dependencies**: Checks if key libraries from the virtual environment can be imported successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "daa1006f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "daa1006f",
        "outputId": "7cec418a-3334-4617-9f7d-2ac719a38157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted.\n"
          ]
        }
      ],
      "source": [
        "# Connect to Google Drive (for Colab users)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print('Google Drive mounted.')\n",
        "except ImportError:\n",
        "    print('Not running in Colab, skipping Google Drive mount.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a48bcdf3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a48bcdf3",
        "outputId": "8e5d322a-db75-4885-f0fe-9b6a725827ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating point-cloud-compression...\n",
            "From https://github.com/rhmes/point-cloud-compression\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "Current working directory: /content/drive/MyDrive/projects/point-cloud-compression\n"
          ]
        }
      ],
      "source": [
        "# Clone repo only if not already cloned\n",
        "import os\n",
        "repo_url = 'https://github.com/rhmes/point-cloud-compression.git'\n",
        "parent_dir = '/content/drive/MyDrive/projects'\n",
        "repo_dir = os.path.join(parent_dir, 'point-cloud-compression')\n",
        "os.makedirs(parent_dir, exist_ok=True)\n",
        "os.chdir(parent_dir)\n",
        "if not os.path.exists(repo_dir):\n",
        "  print('Cloning point-cloud-compression...')\n",
        "  !git clone {repo_url}\n",
        "else:\n",
        "  print('Updating point-cloud-compression...')\n",
        "  !cd {repo_dir} && git pull origin main\n",
        "os.chdir(repo_dir)\n",
        "print(f'Current working directory: {os.getcwd()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3fd280",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1a3fd280",
        "outputId": "9eae9a39-780b-43a8-f372-7c9f77db126c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up GPU virtual environment...\n",
            "Virtual environment already exists at /content/drive/MyDrive/projects/point-cloud-compression/venv_gpu. Skipping setup script.\n"
          ]
        }
      ],
      "source": [
        "# Set this variable to True to install GPU dependencies, False for CPU\n",
        "use_gpu = True # @param {type:\"boolean\"}\n",
        "\n",
        "# Install dependencies based on the use_gpu flag\n",
        "\n",
        "import os\n",
        "\n",
        "if use_gpu:\n",
        "  print(\"Setting up GPU virtual environment...\")\n",
        "  venv_dir = 'venv_gpu'\n",
        "  requirements_file = 'requirements_gpu.txt'\n",
        "else:\n",
        "  print(\"Setting up CPU virtual environment...\")\n",
        "  venv_dir = 'venv_cpu'\n",
        "  requirements_file = 'requirements_cpu.txt'\n",
        "\n",
        "# Check if the virtual environment directory already exists\n",
        "# !sudo update-alternatives --set python3 /usr/bin/python3.10\n",
        "\n",
        "full_venv_dir = os.path.join(repo_dir, venv_dir)\n",
        "venv_activate = full_venv_dir+ \"/bin/activate\"\n",
        "venv_python = full_venv_dir+ \"/bin/python\"\n",
        "if not os.path.exists(full_venv_dir):\n",
        "  print(f'No virtual environment found at {venv_dir}. Running setup script...')\n",
        "  # Run the setup script\n",
        "  %pip install virtualenv\n",
        "  !virtualenv -p /usr/bin/python3.10 {full_venv_dir}\n",
        "  !source {venv_activate}\n",
        "  !echo \"Activating virtual environment...\"\n",
        "  !echo \"Installing dependencies from {requirements_file}...\"\n",
        "  !echo venv_python={venv_python}\n",
        "\n",
        "  !{venv_python} -m pip install -r {requirements_file}\n",
        "\n",
        "  !echo \"Virtual environment setup complete.\"\n",
        "\n",
        "else:\n",
        "    print(f'Virtual environment already exists at {full_venv_dir}. Skipping setup script.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a733dc6",
      "metadata": {
        "collapsed": true,
        "id": "8a733dc6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Define the virtual environment path\n",
        "venv_python = os.path.join(full_venv_dir, 'bin', 'python')\n",
        "venv_site_packages = os.path.join(full_venv_dir, 'lib', 'python3.10', 'site-packages') # Assuming python3.10\n",
        "\n",
        "print(f\"Virtual environment path: {full_venv_dir}\")\n",
        "print(f\"Virtual environment site-packages path: {venv_site_packages}\")\n",
        "print(f\"Virtual environment python executable: {venv_python}\")\n",
        "\n",
        "# Check if the site-packages directory exists and list its contents\n",
        "if os.path.exists(venv_site_packages):\n",
        "    print(f\"\\nListing contents of {venv_site_packages}:\")\n",
        "    # Limit the output to avoid flooding the display if there are many files\n",
        "    contents = os.listdir(venv_site_packages)\n",
        "    print(f\"Found {len(contents)} items.\")\n",
        "else:\n",
        "    print(f\"\\nSite-packages directory not found at {venv_site_packages}\")\n",
        "\n",
        "# Attempt to import the libraries using the virtual environment's python\n",
        "print(\"\\nAttempting to import libraries using venv python:\")\n",
        "import subprocess\n",
        "\n",
        "libraries_to_check = ['numpy', 'open3d', 'pytorch3d', 'torch', 'pyntcloud']\n",
        "\n",
        "for lib in libraries_to_check:\n",
        "    try:\n",
        "        # Use the virtual environment's python to run a command that imports the library\n",
        "        command = [venv_python, '-c', f'import {lib}; print(f\"{lib} imported successfully\")']\n",
        "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
        "        print(result.stdout.strip())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Virtual environment python executable not found at {venv_python}\")\n",
        "        break # Stop if venv python is not found\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"{lib} import failed: {e.stderr.strip()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while checking {lib}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84cd86f4",
      "metadata": {
        "id": "84cd86f4"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "This step involves downloading and preparing the datasets required for the point cloud compression project. Specifically, it downloads the pre-converted ModelNet40 and ShapeNet point cloud datasets from Google Drive if they are not already present in the `data` directory, and extracts the contents of the downloaded zip files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0f0d2ee8",
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0f0d2ee8",
        "outputId": "bb917e53-b6d5-4ba6-ac35-2b27e591d325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rarfile not found, installing...\n",
            "ModelNet40 dataset already exists. Skipping download.\n",
            "ShapeNet dataset already exists. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "# Download ModelNet40 and ShapeNet datasets from Google Drive (Option 1 from README)\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "def download_from_gdrive(url, output_path):\n",
        "    try:\n",
        "        import gdown\n",
        "    except ImportError:\n",
        "        os.system('pip install gdown')\n",
        "        import gdown\n",
        "    gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "# Install rarfile for extracting .rar files\n",
        "try:\n",
        "    import rarfile\n",
        "except ImportError:\n",
        "    print(\"rarfile not found, installing...\")\n",
        "    os.system('pip install rarfile')\n",
        "    import rarfile\n",
        "\n",
        "# ModelNet40\n",
        "repo_root = os.getcwd()\n",
        "modelnet_url = 'https://drive.google.com/uc?id=1Isa8seckZ9oNzstlE7VZcd6wVVx8LdMF'\n",
        "# Assuming the file is actually a .rar despite the variable name\n",
        "modelnet_archive = 'ModelNet40_pc_8192.rar'\n",
        "if not os.path.exists(repo_root + '/data/ModelNet40_pc_01_8192p'):\n",
        "    print('Downloading ModelNet40 pre-converted point clouds...')\n",
        "    download_from_gdrive(modelnet_url, modelnet_archive)\n",
        "    try:\n",
        "        with rarfile.RarFile(modelnet_archive, 'r') as rf:\n",
        "            rf.extractall('data')\n",
        "        os.remove(modelnet_archive)\n",
        "        print('ModelNet40 download and extraction complete.')\n",
        "    except rarfile.BadRarFile as e:\n",
        "        print(f\"Error extracting ModelNet40: {e}\")\n",
        "        print(\"Please ensure the downloaded file is a valid .rar archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during ModelNet40 extraction: {e}\")\n",
        "\n",
        "else:\n",
        "    print('ModelNet40 dataset already exists. Skipping download.')\n",
        "\n",
        "# ShapeNet\n",
        "shapenet_url = 'https://drive.google.com/uc?id=1OzaU01kolBpfRRD0zKESYh67Hh2s2dbD'\n",
        "# Assuming the file is actually a .rar despite the variable name\n",
        "shapenet_archive = 'ShapeNet_pc_2048.rar'\n",
        "if not os.path.exists(repo_root + '/data/ShapeNet_pc_01_2048p'):\n",
        "    print('Downloading ShapeNet pre-converted point clouds...')\n",
        "    download_from_gdrive(shapenet_url, shapenet_archive)\n",
        "    try:\n",
        "        with rarfile.RarFile(shapenet_archive, 'r') as rf:\n",
        "            rf.extractall('data')\n",
        "        os.remove(shapenet_archive)\n",
        "        print('ShapeNet download and extraction complete.')\n",
        "    except rarfile.BadRarFile as e:\n",
        "        print(f\"Error extracting ShapeNet: {e}\")\n",
        "        print(\"Please ensure the downloaded file is a valid .rar archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during ShapeNet extraction: {e}\")\n",
        "else:\n",
        "    print('ShapeNet dataset already exists. Skipping download.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc735bd8",
      "metadata": {
        "id": "dc735bd8"
      },
      "source": [
        "## Repository Scripts Overview\n",
        "\n",
        "This section describes the core scripts from the `point-cloud-compression` repository that are executed in this notebook to demonstrate the project's functionality:\n",
        "\n",
        "\n",
        "1.  **`train.py`**: This script trains the autoencoder model on the ModelNet40 training set. It takes input point cloud file paths, an output directory for the trained model, and the patch size (K) as arguments.\n",
        "2.  **`compress.py`**: This script is used to compress point cloud files. It takes input point cloud file paths, an output directory for compressed files, and a trained model path as arguments.\n",
        "3.  **`decompress.py`**: This script performs the decompression of the compressed point cloud files generated by `compress.py`. It requires the directory containing compressed files, an output directory for decompressed files, and the trained model path.\n",
        "4.  **`eval.py`**: This script evaluates the performance of the compression and decompression process. It compares the original point clouds with the decompressed ones using metrics such as PSNR (Peak Signal-to-Noise Ratio), Chamfer distance, and bits per point (bpp). The evaluation results are typically saved to a CSV file.\n",
        "5.  **`visualize.py`**: This script is used to visualize the evaluation metrics generated by `eval.py`. It reads the results from the CSV file and generates plots or figures to help analyze the compression performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f4388e25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f4388e25",
        "outputId": "8241f9d0-89b2-4e55-e317-7d821ec72d4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files in 'data': ['ShapeNet_pc_01_2048p', 'ModelNet40_pc_01_8192p', '.gitkeep']\n"
          ]
        }
      ],
      "source": [
        "# Example core functionality: List files in the data directory\n",
        "\n",
        "def list_data_files(data_dir='data'):\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Directory '{data_dir}' does not exist.\")\n",
        "        return []\n",
        "    files = os.listdir(data_dir)\n",
        "    print(f\"Files in '{data_dir}':\", files)\n",
        "    return files\n",
        "\n",
        "# Run the function\n",
        "data_files = list_data_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b929b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a6b929b9",
        "outputId": "0285e45b-b75d-48ff-89e2-12f825fcab0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing on device (gpu/cpu): cuda\n",
            "loading point clouds...\n",
            " 32% 3137/9843 [07:42<19:10,  5.83it/s]"
          ]
        }
      ],
      "source": [
        "# Train the autoencoder model on the ModelNet40 training set\n",
        "# Use the virtual environment's python to run the script\n",
        "!{venv_python} train.py  --train_glob './data/ModelNet40_pc_01_8192p/**/train/*.ply' --model_save_folder './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f7663b",
      "metadata": {
        "id": "50f7663b"
      },
      "outputs": [],
      "source": [
        "# Train the Pointnet++-FoldingNet-AE model on the ModelNet40 dataset\n",
        "# Use the virtual environment's python to run the script\n",
        "!{venv_python} train.py  --train_glob './data/ModelNet40_pc_01_8192p/**/train/*.ply' --model_save_folder ./model/K256-PPPE-AE/ --model PPPF-AE --lr 0.0002 --batch_size 1 --lamda 1e-5 --rate_loss_enable_step 20000 --lr_decay 0.5 --lr_decay_steps 40000 --max_steps 120000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9EVS2JEsCh0d",
      "metadata": {
        "id": "9EVS2JEsCh0d"
      },
      "outputs": [],
      "source": [
        "# Train the Pointnet++-PCN-AE model on the ModelNet40 dataset\n",
        "# Use the virtual environment's python to run the script\n",
        "!{venv_python} train_pppe_pcd_ae.py --lr 0.0001 --step_window 700 --batch_size 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33385f34",
      "metadata": {
        "id": "33385f34"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Compress point cloud test files using the trained model\n",
        "!{venv_python} compress.py './data/ModelNet40_pc_01_8192p/**/test/*.ply' './data/ModelNet40_K256_compressed' './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "406dce46",
      "metadata": {
        "id": "406dce46"
      },
      "outputs": [],
      "source": [
        "# Decompress the compressed point cloud files\n",
        "!{venv_python} decompress.py './data/ModelNet40_K256_compressed' './data/ModelNet40_K256_decompressed' './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec7f1ca",
      "metadata": {
        "id": "1ec7f1ca"
      },
      "outputs": [],
      "source": [
        "# Evaluate the compression results using PSNR, Chamfer distance, and bpp metrics\n",
        "!{venv_python} eval.py --input_glob './data/ModelNet40_pc_01_8192p/**/test/*.ply' --compressed_path './data/ModelNet40_K256_compressed' --decompressed_path './data/ModelNet40_K256_decompressed' --output_file './eval/ModelNet40_K256.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06630165",
      "metadata": {
        "id": "06630165"
      },
      "outputs": [],
      "source": [
        "# Visualize evaluation metrics and save plots\n",
        "!{venv_python} visualize.py --csv './eval/ModelNet40_K256.csv' --outdir './figure/'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
