{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhmes/point-cloud-compression/blob/main/point_cloud_compression_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4161fdd0",
      "metadata": {
        "id": "4161fdd0"
      },
      "source": [
        "# Point Cloud Compression Demo\n",
        "\n",
        "This notebook demonstrates installation, core functionality, and testing for the point cloud compression project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa1006f",
      "metadata": {
        "id": "daa1006f"
      },
      "outputs": [],
      "source": [
        "# Connect to Google Drive (for Colab users)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print('Google Drive mounted.')\n",
        "except ImportError:\n",
        "    print('Not running in Colab, skipping Google Drive mount.')\n",
        "\n",
        "# Check if venv_cpu or venv_gpu exists, run setup if not\n",
        "import os\n",
        "if not (os.path.exists('venv_cpu') or os.path.exists('venv_gpu')):\n",
        "    print('No virtual environment found. Running setup...')\n",
        "    os.system('bash venv_gpu_setup.sh')  # Change to venv_cpu_setup.sh if needed\n",
        "else:\n",
        "    print('Virtual environment already exists. Skipping setup.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f0d2ee8",
      "metadata": {
        "id": "0f0d2ee8"
      },
      "outputs": [],
      "source": [
        "# Check if datasets exist and download if missing\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "def check_and_download_dataset(dataset_dir, url=None):\n",
        "    if os.path.exists(dataset_dir) and os.listdir(dataset_dir):\n",
        "        print(f\"Dataset found in '{dataset_dir}'.\")\n",
        "        return True\n",
        "    if url:\n",
        "        print(f\"Downloading dataset to '{dataset_dir}'...\")\n",
        "        os.makedirs(dataset_dir, exist_ok=True)\n",
        "        # Example: download and extract logic (customize for your dataset)\n",
        "        zip_path = os.path.join(dataset_dir, 'dataset.zip')\n",
        "        urllib.request.urlretrieve(url, zip_path)\n",
        "        import zipfile\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(dataset_dir)\n",
        "        os.remove(zip_path)\n",
        "        print(\"Download and extraction complete.\")\n",
        "        return True\n",
        "    print(f\"Dataset not found and no URL provided for '{dataset_dir}'.\")\n",
        "    return False\n",
        "\n",
        "# Example usage (customize URLs as needed):\n",
        "check_and_download_dataset('data/ModelNet40', url=None)  # Add URL if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4388e25",
      "metadata": {
        "id": "f4388e25"
      },
      "outputs": [],
      "source": [
        "# Example core functionality: List files in the data directory\n",
        "\n",
        "def list_data_files(data_dir='data'):\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Directory '{data_dir}' does not exist.\")\n",
        "        return []\n",
        "    files = os.listdir(data_dir)\n",
        "    print(f\"Files in '{data_dir}':\", files)\n",
        "    return files\n",
        "\n",
        "# Run the function\n",
        "data_files = list_data_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33385f34",
      "metadata": {
        "id": "33385f34"
      },
      "outputs": [],
      "source": [
        "# Compress point cloud test files using the trained model\n",
        "!python compress.py './data/ModelNet40_pc_01_8192p/**/test/*.ply' './data/ModelNet40_K256_compressed' './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "406dce46",
      "metadata": {
        "id": "406dce46"
      },
      "outputs": [],
      "source": [
        "# Decompress the compressed point cloud files\n",
        "!python decompress.py './data/ModelNet40_K256_compressed' './data/ModelNet40_K256_decompressed' './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec7f1ca",
      "metadata": {
        "id": "1ec7f1ca"
      },
      "outputs": [],
      "source": [
        "# Evaluate the compression results using PSNR, Chamfer distance, and bpp metrics\n",
        "!python eval.py './data/ModelNet40_pc_01_8192p/**/test/*.ply' './data/ModelNet40_K256_compressed' './data/ModelNet40_K256_decompressed' './eval/ModelNet40_K256.csv' '../geo_dist/build/pc_error'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06630165",
      "metadata": {
        "id": "06630165"
      },
      "outputs": [],
      "source": [
        "# Visualize evaluation metrics and save plots\n",
        "!python visualize.py --csv './eval/ModelNet40_K256.csv' --outdir './figure/'"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}