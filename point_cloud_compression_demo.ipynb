{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhmes/point-cloud-compression/blob/main/point_cloud_compression_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4161fdd0",
      "metadata": {
        "id": "4161fdd0"
      },
      "source": [
        "# Point Cloud Compression Demo\n",
        "\n",
        "This notebook demonstrates installation, core functionality, and testing for the point cloud compression project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a81a4a6b"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "This section prepares the environment for the point cloud compression project:\n",
        "\n",
        "1.  **Mount Google Drive**: Connects Colab to Google Drive for persistent storage of the repository and virtual environment.\n",
        "2.  **Clone/Update Repository**: Clones the project repository from GitHub to Google Drive or updates it if it already exists.\n",
        "3.  **Virtual Environment Setup**: Creates and populates a virtual environment (`venv_gpu` or `venv_cpu`) in Google Drive using a setup script, only if it doesn't exist.\n",
        "4.  **Prepare Environment**: Modifies Python's `sys.path` to include the virtual environment's packages, allowing the notebook to use installed dependencies across sessions.\n",
        "5.  **Verify Dependencies**: Checks if key libraries from the virtual environment can be imported successfully."
      ],
      "id": "a81a4a6b"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "daa1006f",
      "metadata": {
        "id": "daa1006f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7ec087f2-e29d-43af-86a3-10462f3626f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted.\n"
          ]
        }
      ],
      "source": [
        "# Connect to Google Drive (for Colab users)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print('Google Drive mounted.')\n",
        "except ImportError:\n",
        "    print('Not running in Colab, skipping Google Drive mount.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a48bcdf3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a48bcdf3",
        "outputId": "6cce6211-3807-4851-a10b-eeeefe465875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/drive/MyDrive/projects/point-cloud-compression\n"
          ]
        }
      ],
      "source": [
        "# Clone repo only if not already cloned\n",
        "import os\n",
        "repo_url = 'https://github.com/rhmes/point-cloud-compression.git'\n",
        "parent_dir = '/content/drive/MyDrive/projects'\n",
        "repo_dir = os.path.join(parent_dir, 'point-cloud-compression')\n",
        "os.makedirs(parent_dir, exist_ok=True)\n",
        "os.chdir(parent_dir)\n",
        "if not os.path.exists('point-cloud-compression'):\n",
        "    os.system(f'git clone {repo_url}')\n",
        "else:\n",
        "    os.system(f'cd {repo_dir} && git pull')\n",
        "os.chdir(repo_dir)\n",
        "print(f'Current working directory: {os.getcwd()}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1a3fd280",
        "outputId": "dec59c25-5759-4ed2-d8a7-f130f15c8f44"
      },
      "source": [
        "# Set this variable to True to set up GPU virtual environment, False for CPU\n",
        "use_gpu = True # @param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Determine the virtual environment directory and setup script\n",
        "if use_gpu:\n",
        "    print(\"Checking for GPU virtual environment...\")\n",
        "    venv_dir = 'venv_gpu'\n",
        "    requirements_file = 'requirements_gpu.txt'\n",
        "    setup_script = 'venv_gpu_setup.sh'\n",
        "else:\n",
        "    print(\"Checking for CPU virtual environment...\")\n",
        "    venv_dir = 'venv_cpu'\n",
        "    requirements_file = 'requirements_cpu.txt'\n",
        "    setup_script = 'venv_cpu_setup.sh'\n",
        "\n",
        "\n",
        "# Construct the full path to the virtual environment directory\n",
        "repo_root = os.getcwd()\n",
        "full_venv_path = os.path.join(repo_root, venv_dir)\n",
        "\n",
        "\n",
        "# Check if the virtual environment directory already exists\n",
        "if os.path.exists(full_venv_path):\n",
        "    print(f'Virtual environment already exists at {full_venv_path}. Skipping dependency installation.')\n",
        "else:\n",
        "    print(f'Virtual environment not found at {full_venv_path}. Proceeding with setup and installation.')\n",
        "\n",
        "    # If the venv doesn't exist, first try running the setup script\n",
        "    print(f\"Running setup script: {setup_script}\")\n",
        "\n",
        "    !bash {setup_script}\n",
        "\n",
        "    print(\"Installing additional dependencies...\")\n",
        "\n",
        "    # Install Open3D\n",
        "    !pip install open3d\n",
        "\n",
        "    # Install PyTorch3D from source\n",
        "    !pip install \"git+https://github.com/facebookresearch/pytorch3d.git\"\n",
        "\n",
        "    print(\"Dependency installation complete.\")"
      ],
      "id": "1a3fd280",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for GPU virtual environment...\n",
            "Virtual environment already exists at /content/drive/MyDrive/projects/point-cloud-compression/venv_gpu. Skipping dependency installation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "collapsed": true,
        "id": "5b3b69d2",
        "outputId": "6958bf94-7a5d-466a-c7e9-d840c634b849"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Prepare the virtual environment's site-packages\n",
        "\n",
        "venv_dir_name = venv_dir\n",
        "\n",
        "# Construct the path to the virtual environment's site-packages directory\n",
        "repo_root = os.getcwd()\n",
        "venv_path = os.path.join(repo_root, venv_dir_name)\n",
        "\n",
        "# Common site-packages paths relative to the venv directory\n",
        "site_packages_paths_candidates = [\n",
        "    os.path.join(venv_path, 'lib', f'python{sys.version_info.major}.{sys.version_info.minor}', 'site-packages'),\n",
        "    os.path.join(venv_path, 'lib64', f'python{sys.version_info.major}.{sys.version_info.minor}', 'site-packages'), # For some systems\n",
        "    os.path.join(venv_path, 'Lib', 'site-packages'), # For Windows\n",
        "]\n",
        "\n",
        "site_packages_path = None\n",
        "for candidate in site_packages_paths_candidates:\n",
        "    if os.path.exists(candidate):\n",
        "        site_packages_path = candidate\n",
        "        break\n",
        "\n",
        "if site_packages_path and site_packages_path not in sys.path:\n",
        "    sys.path.insert(0, site_packages_path)\n",
        "    print(f\"Added {site_packages_path} to sys.path\")\n",
        "elif site_packages_path:\n",
        "     print(f\"{site_packages_path} already in sys.path\")\n",
        "else:\n",
        "    print(f\"Could not find site-packages directory in {venv_path}\")\n",
        "\n",
        "# Verify that packages from the venv can be imported (optional)\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"Successfully imported torch (version: {torch.__version__})\")\n",
        "except ImportError:\n",
        "    print(\"Could not import torch. Ensure virtual environment is set up correctly.\")\n",
        "\n",
        "try:\n",
        "    import open3d\n",
        "    print(\"Successfully imported open3d\")\n",
        "except ImportError:\n",
        "    print(\"Could not import open3d. Ensure virtual environment is set up correctly.\")\n",
        "\n",
        "try:\n",
        "    import pytorch3d\n",
        "    print(\"Successfully imported pytorch3d\")\n",
        "except ImportError:\n",
        "    print(\"Could not import pytorch3d. Ensure virtual environment is set up correctly.\")"
      ],
      "id": "5b3b69d2",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added /content/drive/MyDrive/projects/point-cloud-compression/venv_gpu/lib/python3.11/site-packages to sys.path\n",
            "Successfully imported torch (version: 2.3.0+cu121)\n",
            "Successfully imported open3d\n",
            "Successfully imported pytorch3d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84cd86f4"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "This step involves downloading and preparing the datasets required for the point cloud compression project. Specifically, it downloads the pre-converted ModelNet40 and ShapeNet point cloud datasets from Google Drive if they are not already present in the `data` directory, and extracts the contents of the downloaded zip files."
      ],
      "id": "84cd86f4"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0f0d2ee8",
      "metadata": {
        "id": "0f0d2ee8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "collapsed": true,
        "outputId": "b39ed911-1eb5-42e7-ca4c-3ba5c81fd07e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ModelNet40 pre-converted point clouds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Isa8seckZ9oNzstlE7VZcd6wVVx8LdMF\n",
            "From (redirected): https://drive.google.com/uc?id=1Isa8seckZ9oNzstlE7VZcd6wVVx8LdMF&confirm=t&uuid=76e1eecb-6884-446e-a9d1-eabd0d7603b3\n",
            "To: /content/drive/MyDrive/projects/point-cloud-compression/ModelNet40_pc_8192.rar\n",
            "100%|██████████| 963M/963M [00:06<00:00, 141MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelNet40 download and extraction complete.\n",
            "Downloading ShapeNet pre-converted point clouds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1OzaU01kolBpfRRD0zKESYh67Hh2s2dbD\n",
            "From (redirected): https://drive.google.com/uc?id=1OzaU01kolBpfRRD0zKESYh67Hh2s2dbD&confirm=t&uuid=90a61e30-ad1a-4e34-8654-ce0614019680\n",
            "To: /content/drive/MyDrive/projects/point-cloud-compression/ShapeNet_pc_2048.rar\n",
            "100%|██████████| 47.0M/47.0M [00:00<00:00, 74.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ShapeNet download and extraction complete.\n"
          ]
        }
      ],
      "source": [
        "# Download ModelNet40 and ShapeNet datasets from Google Drive (Option 1 from README)\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "def download_from_gdrive(url, output_path):\n",
        "    try:\n",
        "        import gdown\n",
        "    except ImportError:\n",
        "        os.system('pip install gdown')\n",
        "        import gdown\n",
        "    gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "# Install rarfile for extracting .rar files\n",
        "try:\n",
        "    import rarfile\n",
        "except ImportError:\n",
        "    print(\"rarfile not found, installing...\")\n",
        "    os.system('pip install rarfile')\n",
        "    import rarfile\n",
        "\n",
        "# ModelNet40\n",
        "modelnet_url = 'https://drive.google.com/uc?id=1Isa8seckZ9oNzstlE7VZcd6wVVx8LdMF'\n",
        "# Assuming the file is actually a .rar despite the variable name\n",
        "modelnet_archive = 'ModelNet40_pc_8192.rar'\n",
        "if not os.path.exists('data/ModelNet40_pc_8192'):\n",
        "    print('Downloading ModelNet40 pre-converted point clouds...')\n",
        "    download_from_gdrive(modelnet_url, modelnet_archive)\n",
        "    try:\n",
        "        with rarfile.RarFile(modelnet_archive, 'r') as rf:\n",
        "            rf.extractall('data')\n",
        "        os.remove(modelnet_archive)\n",
        "        print('ModelNet40 download and extraction complete.')\n",
        "    except rarfile.BadRarFile as e:\n",
        "        print(f\"Error extracting ModelNet40: {e}\")\n",
        "        print(\"Please ensure the downloaded file is a valid .rar archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during ModelNet40 extraction: {e}\")\n",
        "\n",
        "else:\n",
        "    print('ModelNet40 dataset already exists. Skipping download.')\n",
        "\n",
        "# ShapeNet\n",
        "shapenet_url = 'https://drive.google.com/uc?id=1OzaU01kolBpfRRD0zKESYh67Hh2s2dbD'\n",
        "# Assuming the file is actually a .rar despite the variable name\n",
        "shapenet_archive = 'ShapeNet_pc_2048.rar'\n",
        "if not os.path.exists('data/ShapeNet_pc_2048'):\n",
        "    print('Downloading ShapeNet pre-converted point clouds...')\n",
        "    download_from_gdrive(shapenet_url, shapenet_archive)\n",
        "    try:\n",
        "        with rarfile.RarFile(shapenet_archive, 'r') as rf:\n",
        "            rf.extractall('data')\n",
        "        os.remove(shapenet_archive)\n",
        "        print('ShapeNet download and extraction complete.')\n",
        "    except rarfile.BadRarFile as e:\n",
        "        print(f\"Error extracting ShapeNet: {e}\")\n",
        "        print(\"Please ensure the downloaded file is a valid .rar archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during ShapeNet extraction: {e}\")\n",
        "else:\n",
        "    print('ShapeNet dataset already exists. Skipping download.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc735bd8"
      },
      "source": [
        "## Repository Scripts Overview\n",
        "\n",
        "This section describes the core scripts from the `point-cloud-compression` repository that are executed in this notebook to demonstrate the project's functionality:\n",
        "\n",
        "1.  **`compress.py`**: This script is used to compress point cloud files. It takes input point cloud file paths, an output directory for compressed files, and a trained model path as arguments.\n",
        "2.  **`decompress.py`**: This script performs the decompression of the compressed point cloud files generated by `compress.py`. It requires the directory containing compressed files, an output directory for decompressed files, and the trained model path.\n",
        "3.  **`eval.py`**: This script evaluates the performance of the compression and decompression process. It compares the original point clouds with the decompressed ones using metrics such as PSNR (Peak Signal-to-Noise Ratio), Chamfer distance, and bits per point (bpp). The evaluation results are typically saved to a CSV file.\n",
        "4.  **`visualize.py`**: This script is used to visualize the evaluation metrics generated by `eval.py`. It reads the results from the CSV file and generates plots or figures to help analyze the compression performance."
      ],
      "id": "dc735bd8"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f4388e25",
      "metadata": {
        "id": "f4388e25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "08857c2d-355b-4ba0-f85d-9bcc337e1fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in 'data': ['.gitkeep', 'ModelNet40_pc_01_8192p', 'ShapeNet_pc_01_2048p']\n"
          ]
        }
      ],
      "source": [
        "# Example core functionality: List files in the data directory\n",
        "\n",
        "def list_data_files(data_dir='data'):\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Directory '{data_dir}' does not exist.\")\n",
        "        return []\n",
        "    files = os.listdir(data_dir)\n",
        "    print(f\"Files in '{data_dir}':\", files)\n",
        "    return files\n",
        "\n",
        "# Run the function\n",
        "data_files = list_data_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33385f34",
      "metadata": {
        "id": "33385f34"
      },
      "outputs": [],
      "source": [
        "# Compress point cloud test files using the trained model\n",
        "!python compress.py './data/ModelNet40_pc_01_8192p/**/test/*.ply' './data/ModelNet40_K256_compressed' './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "406dce46",
      "metadata": {
        "id": "406dce46"
      },
      "outputs": [],
      "source": [
        "# Decompress the compressed point cloud files\n",
        "!python decompress.py './data/ModelNet40_K256_compressed' './data/ModelNet40_K256_decompressed' './model/K256' --K 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec7f1ca",
      "metadata": {
        "id": "1ec7f1ca"
      },
      "outputs": [],
      "source": [
        "# Evaluate the compression results using PSNR, Chamfer distance, and bpp metrics\n",
        "!python eval.py './data/ModelNet40_pc_01_8192p/**/test/*.ply' './data/ModelNet40_K256_compressed' './data/ModelNet40_K256_decompressed' './eval/ModelNet40_K256.csv' '../geo_dist/build/pc_error'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06630165",
      "metadata": {
        "id": "06630165"
      },
      "outputs": [],
      "source": [
        "# Visualize evaluation metrics and save plots\n",
        "!python visualize.py --csv './eval/ModelNet40_K256.csv' --outdir './figure/'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}